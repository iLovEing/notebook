# [声纹识别基础](https://github.com/iLovEing/notebook/issues/23)

声纹的概念来自于指纹，广义上讲，可以将一个\一类声音和其他声音区分开来的特征，都可以认为是声纹特征。这里主要记录声纹识别相关的技术发展和工程应用
> 摘录自《声纹技术：从核心算法到工程实践》



---

## 声纹技术的前世今生
![screenshot-20230816-144619](https://github.com/iLovEing/notebook/assets/109459299/ba823866-098f-44f7-bd54-a911daf3d3c7)

1. 早期声纹技术使用直观的向量化-匹配差异的思路，将音频转化为时频谱矩阵或**梅尔倒谱系数**，继而使用向量/矩阵求的距离的手段来对音频识别分类。
2. 1995~2006年，在GMM出现后，**GMM-HMM**, **GMM-UBM**, **GMM-SVM** 等技术相继运用到声纹识别上，这个阶段是声纹识别的高斯混合模型时代。
3. 2007-2011 **联合因子分析(JFA)**与**i-vector**被提出，其充分考虑了信道的变化因素，将每个说话人的模型从高斯混合模型的多个分量中，映射到了更加低维的表征空间，从而克服了高斯分量互相独立的局限性。
4. 14年以后，随着深度学习技术的爆发，**LSTM**、**attention**等网络结构被越来越多地用到声音处理上。

> 下面对其中重要的概念和方法做简单的介绍

---

## 音频相关的基础概念

### 1. 声学基础
- **正弦波**
$y = Asin(2\pi fx + \varphi )$
其中，*f* 为频率，  $2\pi f$ 为角频率， $2\pi fx + \varphi$ 为相位， $\varphi$ 为初始相位

- **频谱**
复杂的波形可以通过[fft](https://github.com/iLovEing/notebook/issues/11)分解为许多个正弦波的叠加，因此我们可以画出这样一张图，其横轴为这些正弦波分量的频率，纵轴为这些正弦波分量的振幅,这样的图就是频谱。

- **基音**与**基频**
复杂声音中最低且通常情况下最强的频率，我理解为声音的基础“音调”

- **功率**与**声强**
假设周期信号为 $y=f(t)$ ，声音的功率可以表示为：
$P=\frac{1}{T}\int_{0}^{T}(f(t))^{2}dt$
声强定义为：  
$L_{dB} =10\log_{10}{\frac{P}{P_{0}}} $单位为分贝， $P_{0}$ 为人耳能听到的最小分贝

- **非线性刻度**
人耳对声音的感知是非线性的，这种非线性主要体现在两方面：对频率感知的非线性，以及对声强感知的非线性。经典的频率刻度有：
    1. 巴克刻度：将人耳可以听见的频率范围分为24个频率群，每个巴克覆盖了一定的频率群，每一个频率群由其中心频率、截止频率和带宽确定：
![image](https://github.com/iLovEing/notebook/assets/109459299/833bca51-09b5-436e-9162-5a6adb4c299a)
    2. 梅尔刻度：梅尔刻度是一个单调递增的连续映射，梅尔 $m$ 与频率 $f$ 之间的映射关系为：
    $m=2595\log_{10}{(1+\frac{f}{700})} =1127\ln_{}{(1+\frac{f}{700})} $
频率与梅尔之间的换算以1000Hz 为参考点。 $f$ =1000Hz 时， $m$ =1000mel。不同底数的对数函数，其前面的系数可以由参考点确定。
![image](https://github.com/iLovEing/notebook/assets/109459299/760fa638-749a-4717-bae7-3ba2822f115a)


---

### 2. 数字信号-计算机如何存储？
将连续的模拟信号转换为离散的数字信号有两个步骤：采样和量化。再使用一定的编码规则转换成最后的音频文件。

- **采样**
采样是指对模拟信号进行周期性采样，给定一个采样率，能重建的周期信号的频率是该采样率的一半（波峰一次，波谷一次）。
![image](https://github.com/iLovEing/notebook/assets/109459299/a6e8c7b4-ded7-41bf-8912-4e50e6ac43df)

- **量化**
信号经过采样之后，得到时间轴上每个点的幅值，在计算机上保存时，会用一定位数的存储空间保存（二进制），比如8位，这时所有的幅值都用这个8位二进制来表示，就是8位量化，量化意味着精度的损失。

- **编码**
量化时具体的映射关系即编码规则，比如最简单的线性脉冲编码(liner PCM)，其在量化时相邻整数表示的信号差值固定，即信号在映射到量化区域上是均匀且线性的。常见的编码规则有：线性编码、非线性编码、自适应编码、差分编码（对相邻信号差做量化），线性预测编码($x[n]=\sum_{i=1}^{p}a_{i}x[n-i]+e[n] $)，频域编码等

---

### 3. 短时分析-音频信号特征分析基础
对一段音频做全局特征分析，比如将整段信号进行ft计算频谱，相当于对音频信号在时间轴上做了某种平滑处理，从而使时间分辨率降为零，这种做法会丢失信号中的大量局部信息。如果音频中有一些噪音片段，那么这些片段会对最后得到的特征造成进一步污染。
全局特征只有当信号十分平稳的时候才是有意义的，而日常的音频信号随时间变化而变化，因此，需要从局部提取特征，在每一个局部的短时间内，可以近似地认为信号是平稳的。这是音频信号特征分析的基础——**短时分析技术**。

- **分帧**
从局部提取特征，就需要将音频按一定长度的窗口分割为帧，分帧时会遇到帧长度，取帧间隔等超参。25 毫秒的帧长度，10 毫秒的帧间隔，是一组语音识别中常用的设置，该设置下帧有重叠。
在分帧后可以计算每帧的特征向量，再使用特征向量做相关任务。有时每一帧输入模型的效果可能不如多帧叠加，引申出**帧叠加**与**帧采样**两种做法：
![image](https://github.com/iLovEing/notebook/assets/109459299/fa302c27-f909-4fbe-aa89-a2dc84e1c3da)

- **窗函数处理**
分帧处理后，每帧的头尾被截断。如果直接对这样的信号做fft，会发生**吉布斯现象**，在不连续点出产生高频分量，导致频谱出现局部峰值。此外，由于周期信号在分帧过程中被截断，会导致频谱在整个频带内发生拖尾现象，这被称为**频谱泄漏**。
为了避免上述不良影响，可以对分帧之后的信号进行加窗处理，将较大的权重赋予靠近窗中心的信号，将接近零的权重赋予靠近窗边缘的信号，减轻分帧时所造成的信号不连续性。常见的有**汉宁窗**和**汉明窗**。（直接用窗口函数乘以信号强度）
![image](https://github.com/iLovEing/notebook/assets/109459299/6ebbc4c5-09c8-4687-b4b8-a330af1b1e56)

---

### 4. 声纹识别常用特征
- **时域特征**
    - 短时能量与短时平均幅值：长度为N的音频帧X，其短时能量为 $E=\sum (X[n])^{2} $，可以用来做简单的语音检测任务，平均幅值原理类似。
    - 短时过零率：可以粗略地反映频率高低。
    - 短时自相关函数：将短时信号延迟k 个采样后，再计算其与信号本身的相关性，便得到短时自相关函数：
 $R(k)=  {\textstyle \sum_{n=0}^{N-1-K}}x[n]\cdot  x[n+k]$
短时自相关函数是根据一组k的值而产生的一组特征，包含了与信号周期有关的重要信息，当k 为周期的倍数时，短时自相关函数会取较大的值。
    - 短时平均幅度差函数：和 短时自相关函数类似，乘法改为减法，计算更为简单。
    - **线性预测倒谱系数(linear predictive cepstral coefficient, LPCC)**：线性倒谱系数和线性预测编码强相关，考虑p阶线性编码中：
$x[n]= {\textstyle \sum_{i=1}^{p}} a_{i}x[n-i]+e[n]$
$e[n]$ 取值为0时，可以用最小二乘法求出一组 $a[i]$ 的值，进而计算线性预测倒谱系数：
$c_{0}=\ln{p}$
$c_{m}=a_{m}+\sum_{k=1}^{m-1}\frac{k}{m}c_{k}a_{m-k}$
$c_{m}=\sum_{k=m-p}^{m-1}\frac{k}{m}c_{k}a_{m-k}$
> 貌似这里和Z变换有关
- **频域特征**
    - 频谱：通过傅里叶变换可以得到频谱，长度为N的信号x[n]使用快速傅里叶便哈可以得到N个频谱幅值特征
![image](https://github.com/iLovEing/notebook/assets/109459299/c4554a38-ee1e-4af7-ae35-525dd33afe13)
    - 倒谱：将频谱的对数作为信号，进行逆傅里叶变换，此时横轴成为倒频率
    - 时频谱：对时间轴上的连续音频帧做傅里叶变换，得到时间为横轴，频率为纵轴，颜色或灰度表示幅度的图像。
    - 功率谱：将频谱或时频谱中的幅值替换为幅值的平方。
- **感知线性预测(perceptual linear prediction，PLP)**
PLP 特征注重对人耳听觉的模拟，PLP 被提出之后，又出现了一些后继改进其中比较有名的是基于带通滤波和均值消减（mean subtraction）的RASTA-PLP 特征[。原始PLP其计算过程为：
    1. 对输入信号进行分帧与加窗处理。原始论文中建议使用20 毫秒的帧长度及汉明窗。
    2. 对每一帧信号进行快速傅里叶变换。如果信号帧长度不是2 的整数次幂，则进行补零处理。变换后取幅度的平方作为功率谱。
    3. 对功率谱进行巴克刻度变换，以校正人耳听觉对于频率的非线性。将变换后的功率谱与一些预先设计好的临界频带（critical-band）滤波器组进行卷积，得到带通滤波后的结果。
    4. 通过等响度曲线（equal-loudness curve）预加重前一步得到的结果，从而校正人耳对不同频率的敏感度。
    5. 用幂函数 $y=x^{1/3}$ 校正人耳对于声强的非线性。
    6. 对前面得到的特征进行逆离散傅里叶变换（inverse discrete Fourier transform，IDFT），从而得到线性预测编码系数。
- **梅尔倒谱系数(mel-frequency cepstral coefficients，MFCC)**
无论是语音识别还是声纹识别，MFCC 都有极为广泛的应用，几乎已经成为业界标准。MFCC 的设计思路与前面提到的感知线性预测有许多相似之处。简要计算过程：
    1. 对音频信号进行预加重处理，从而降低部分高频能量。这一步可以简单采用等式y[n]=x[n]-αx[n-1]进行处理，其中的0.9 ≤α ≤1.0 是预加重系数。注意这一步类似于PLP 的等响度曲线步骤。
    2. 对预加重处理后的信号进行分帧、加窗处理。一般采用汉明窗。
    3. 对每一帧信号进行快速傅里叶变换，得到频谱。
    4. 将频谱通过一组按照梅尔刻度设计好的三角形滤波器组（filterbanks），得到带通滤波后的结果。
    5. 用对数函数校正人耳对于声强的非线性。
    6. 通过逆离散傅里叶变换计算倒谱。
    7. 前一步得到了12 个倒谱系数，再增加一个该帧的能量，得到第13 个特征。通过相邻帧计算这13 个特征的一阶差分及二阶差分，最终得到39 个特征。这39 个特征便是最终的MFCC 特征。
- **功率正则化倒谱系数(power-normalized cepstral coefficients，PNCC)**
PNCC是提出用于用于替代RASTA-PLP或MFCC 的音频特征，其对加性噪声及混响环境下的语音识别有一定提升，在PNCC 的计算过程中，大部分模块都与PLP 及MFCC 十分相似，新颖之处在于，增加了短期和中期处理，通过计算一段时间内的功率来抑制背景中的激励信号。这种做法被称为不对称噪声抑制（asymmetric noise suppression，ANS），其基于这样一个假设：在同一信道内，语音信号的功率变化比噪声的功率变化要快。
![image](https://github.com/iLovEing/notebook/assets/109459299/95852fb9-d40f-4eea-94c4-ba627d79215d)
